{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'tflow' from '/home/oneunreadmail/PycharmProjects/ragequit/tflow.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"/home/oneunreadmail/PycharmProjects/ragequit/\")\n",
    "from importlib import reload\n",
    "import math\n",
    "import itertools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import model_selection, ensemble\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "import tflow \n",
    "reload(tflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class FitError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# функция на работу с уже полученными вероятностями\n",
    "# пока не в деле, но я думаю, что их хитрым образом можно как-то потрогать и улучшить логлосс\n",
    "def expand(array, func=\"\"):\n",
    "    if func == \"drop\":\n",
    "        f = lambda x: x + math.tan((-x + 0.5)*2.7)/10000\n",
    "    elif func == \"tocenter\":\n",
    "        f = lambda x: x - math.sin((x - 0.5)*2*math.pi)/10000\n",
    "    else:\n",
    "        f = lambda x: x\n",
    "    return list(map(f, array))\n",
    "\n",
    "#a = np.arange(0, 1, 0.01)\n",
    "#expand(a, func=\"tocenter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# делает из листа листов листов листов один плоский лист\n",
    "def flatten(seq, container=None):\n",
    "    if container is None:\n",
    "        container = []\n",
    "    for s in seq:\n",
    "        if hasattr(s,'__iter__'):\n",
    "            flatten(s,container)\n",
    "        else:\n",
    "            container.append(s)\n",
    "    return container\n",
    "\n",
    "# считает элементы в листе листов листов листов\n",
    "# тупо чтобы проверить, что мы не потеряли строчки где-то\n",
    "def flat_len(seq):\n",
    "    return len(flatten(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# выдает лист из всех комбинаций элементов входого листа\n",
    "# >>all_perm([\"0\", \"1\", \"2\"])\n",
    "# (['2', '1', '0'], ['1', '0'], ['2', '0'], ['0'], ['2', '1'], ['1'], ['2'], [])\n",
    "def all_perm(list):\n",
    "    if len(list) == 1:\n",
    "        return [list[0]],[]\n",
    "    else:\n",
    "        ap = all_perm(list[1:])\n",
    "        #print(ap)\n",
    "        #print(tuple(lst + [list[0]] for lst in ap))\n",
    "        #print(tuple(lst for lst in ap))\n",
    "        return (tuple(lst + [list[0]] for lst in ap)) + (tuple(lst for lst in ap))\n",
    "\n",
    "#all_perm([\"0\", \"1\", \"2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# сплиттит датафреймы по листу из n индексов\n",
    "# выдает тупл из 2**n элементов, каждый - лист из номеров строк\n",
    "# !ВНИМАНИЕ! если такой комбинации нет, выдаст пустой лист\n",
    "# это будет важно, когда мы будем проверять, можем ли мы обучить модель на каждом случае\n",
    "\n",
    "def df_split(df, indices = []):\n",
    "    if len(indices) > 1:\n",
    "        left =  df_split(df[df[indices[0]] == 0], indices[1:])\n",
    "        right = df_split(df[df[indices[0]] == 1], indices[1:])\n",
    "        assert(len(df.index) == flat_len(left) + flat_len(right))\n",
    "        return left + right\n",
    "    elif len(indices) == 1:\n",
    "        left = list(df[df[indices[0]] == 0].index)\n",
    "        right = list(df[df[indices[0]] == 1].index)\n",
    "        #if not left: return right,\n",
    "        #elif not right: return left,\n",
    "        #else: \n",
    "        return left, right\n",
    "    else:\n",
    "        return list(df.index),\n",
    "\n",
    "#f = x_train[[\"maxPlayerLevel\", \"attLevelsMoreThanMaxLevel\", \"doReturnOnLowerLevels\", \"allAttemptsOnTheHighestLevel\"]][30:50]\n",
    "#f = x_train[[\"maxPlayerLevel\", \"true\"]][30:35]\n",
    "#f.loc[30:35]\n",
    "#g = df_split(f)\n",
    "#g = df_split(f, [])\n",
    "#g = df_split(f, [\"attLevelsMoreThanMaxLevel\"])\n",
    "#g = df_split(f, [\"attLevelsMoreThanMaxLevel\", \"doReturnOnLowerLevels\", \"allAttemptsOnTheHighestLevel\"])\n",
    "#for i in g: print(i)\n",
    "#f.loc[g[1]]\n",
    "\n",
    "#df_split(f, [\"attLevelsMoreThanMaxLevel\", \"doReturnOnLowerLevels\", \"allAttemptsOnTheHighestLevel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# класс с нашими моделями\n",
    "# сразу при вызове забирает данные из !глобальных! x_test, t_train и y_train\n",
    "# учитывает только колонки из листа params (локальный, обязательный)\n",
    "# бьет их на части из листа split (локальный, необязательный)\n",
    "# сразу все фитует и даже считает y_test\n",
    "\n",
    "class ML():\n",
    "    model = None\n",
    "    errors = None\n",
    "    log_loss = None\n",
    "    data = []\n",
    "    test_out = None\n",
    "    \n",
    "    \n",
    "    def __init__(self, model, params, split=[]):\n",
    "        self.params = params\n",
    "        self.model = model\n",
    "        self.split = split\n",
    "        self.form()\n",
    "        self.fit()\n",
    "\n",
    "    \n",
    "    def form(self):\n",
    "        self.train_indices = df_split(x_train, self.split)\n",
    "        self.test_indices = df_split(x_test, self.split)\n",
    "        \n",
    "        # проверяем по всем кускам\n",
    "        # если куска нет в трейне, то провальчик\n",
    "        for i in self.train_indices:\n",
    "            #if (i == []) and (self.test_indices != []):\n",
    "            if not i:    \n",
    "                raise FitError('CANT FIT DIS: some empty train stuff')\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        y1v, y2v, y1p, y2p, y1r, y2r = [], [], [], [], [], []\n",
    "        out, xtr = [], []\n",
    "        \n",
    "        # этот фор исключительно для обсчета сплитов\n",
    "        for i, index in enumerate(self.train_indices):\n",
    "            \n",
    "            # df_split может выдавать пустые массивы\n",
    "            if len(index) == 0: continue\n",
    "            x_train_part = x_train[self.params].loc[index]\n",
    "            # закомментированная строчка возвращает нампи-столбец, но стобцы не любы склерну\n",
    "            #y_train_part = np.array(y_train.returned.loc[index], ndmin=2).T\n",
    "            y_train_part = list(y_train.returned.loc[index])\n",
    "            \n",
    "            # будем резать, будем бить\n",
    "            x1, x2, y1, y2 = train_test_split(x_train_part, y_train_part, test_size = 0.3, random_state=42)\n",
    "            #print(\"x1\\n\", x1)\n",
    "            #print(\"y1\\n\", y1)\n",
    "            self.model.fit(x1, y1)\n",
    "            #try:\n",
    "            #    self.model.fit(x1, y1)\n",
    "            #except ValueError:\n",
    "            #    raise FitError(\"CANT FIT DIS: looks like it needs more rows or something else\")\n",
    "            #print(\"fitted\")\n",
    "            # values\n",
    "            y1v.append(self.model.predict(x1))\n",
    "            y2v.append(self.model.predict(x2))\n",
    "            # probabilities\n",
    "            y1p.append(self.model.predict_proba(x1))\n",
    "            y2p.append(self.model.predict_proba(x2))\n",
    "            # true values\n",
    "            y1r.append(np.array(y1))\n",
    "            y2r.append(np.array(y2))\n",
    "            # applying model to test values\n",
    "            if len(self.test_indices[i]) != 0:\n",
    "                # если есть кусок теста, отфитуем его\n",
    "                x_test_part = x_test[self.params].loc[self.test_indices[i]]\n",
    "                out.append(self.model.predict_proba(x_test_part)[:, 1])\n",
    "        \n",
    "        # будем все собирать\n",
    "        # следующие шесть можно собирать не по порядку, какая в жопу разница, логлоссу все равно\n",
    "        y1v = np.concatenate(y1v)\n",
    "        y2v = np.concatenate(y2v)\n",
    "        y1p = np.concatenate(y1p)\n",
    "        y2p = np.concatenate(y2p)\n",
    "        y1r = np.concatenate(y1r)\n",
    "        y2r = np.concatenate(y2r)\n",
    "\n",
    "        # а вот для ответов на тесты следим за рукой\n",
    "        # пока что out это лист из массивов с предсказаниями для разных кусков\n",
    "        # теперь сделаем из него лист из двумерных массивов вида \"номера корректных строк\", \"предсказания\"\n",
    "        out1 = [np.array([self.test_indices[_], __]) for _, __ in enumerate(out)]\n",
    "        # соберем один двумерный массив из этой кучи\n",
    "        # теперь там куча строк вида [1, 0.44] [2, 0.62], но номера перепутаны, потому что мы резали\n",
    "        out2 = np.concatenate(out1, axis=1).transpose()\n",
    "        # отсортируем\n",
    "        out3 = out2[np.argsort(out2[:, 0])]\n",
    "        # запишем предсказания для теста и вот мы молодцы\n",
    "        self.test_out = out3[:, 1]\n",
    "        \n",
    "        self.errors = np.mean(y1r != y1v), np.mean(y2r != y2v)\n",
    "        self.log_loss = log_loss(y1r, y1p, eps=1e-15), log_loss(y2r, y2p, eps=1e-15)\n",
    "        try:                      \n",
    "            importances = self.model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            #print(len(indices), len(self.params), len(importances))\n",
    "            self.imp = [(self.params[i], importances[i]) for i in indices]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def dump(self):\n",
    "        #a = self.model.predict_proba(data)\n",
    "        np.savetxt(\"data/y_test.csv\", self.test_out, fmt='%10.5f')\n",
    "\n",
    "#XGB = ML(xgb.XGBClassifier(), params, split=params_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xytte = pd.read_csv(\"data/xytte.csv\")\n",
    "xytte[\"true\"] = 1\n",
    "xytt2e = xytte.copy()\n",
    "#for i in range(len(xytte.index)):\n",
    "#    for j in range(i+1, len(xytte.index)):\n",
    "#        #print(xytte.iloc[:, [i]])\n",
    "#        xytt2e[str(i)+\"_\"+str(j)] = xytte.iloc[:, [i]] * xytte.iloc[:, [j]]\n",
    "\n",
    "#print(xytt2e)\n",
    "####################### ВНИМАНИЕ #######################\n",
    "# здесь трейн это которые в задаче трейн, с ответами\n",
    "# тест без ответов\n",
    "# но дальше в алгоритмах трейн будет разбиваться на два подсета\n",
    "# которые будут у меня называться x1-y1 и x2-y2, а по смыслу как раз трейн и тест\n",
    "# надо не перепутать\n",
    "\n",
    "x_train = xytte[xytte.returned == xytte.returned].reset_index(drop=True).drop(\"returned\", axis=1)\n",
    "y_train = xytte[xytte.returned == xytte.returned].reset_index(drop=True)[[\"returned\"]]\n",
    "x_test  = xytte[xytte.returned != xytte.returned].reset_index(drop=True).drop(\"returned\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# проверяем, что нигде не налажали\n",
    "# читаем из файла оригинальные данные\n",
    "\n",
    "x_test_ref = pd.read_csv(\"data/x_test.csv\", sep=\";\", dtype=np.float32)\n",
    "x_train_ref = pd.read_csv(\"data/x_train.csv\", sep=\";\", dtype=np.float32)\n",
    "y_train_ref = pd.read_csv(\"data/y_train.csv\", sep=\";\", dtype=np.float32, header=None, names=[\"returned\"])\n",
    "\n",
    "# заводим функцию на сравнение таблиц\n",
    "\n",
    "def df_equal(df1, df2):\n",
    "    if list(df1.columns) != list(df2.columns): \n",
    "        print(\"Mismatch columns:\")\n",
    "        print(\"df1: \", list(df1.columns))\n",
    "        print(\"df2: \", list(df2.columns))\n",
    "        return False\n",
    "    if list(df1.index) != list(df2.index): \n",
    "        print(\"Mismatch index:\")\n",
    "        print(\"df1: \", list(df1.columns)[0], \":\", list(df1.columns)[-1])\n",
    "        print(\"df2: \", list(df2.columns)[0], \":\", list(df2.columns)[-1])\n",
    "        return False\n",
    "    \n",
    "    ne_stacked = (df1 != df2).stack()\n",
    "    changed = ne_stacked[ne_stacked]\n",
    "    changed.index.names = ['id', 'col']\n",
    "\n",
    "    difference_locations = np.where(df1 != df2)\n",
    "    changed_from = df1.values[difference_locations]\n",
    "    changed_to = df2.values[difference_locations]\n",
    "    diff = pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)\n",
    "    return not diff[diff[\"from\"] - diff[\"to\"] > 0.000001].count()[0]\n",
    "\n",
    "# ассертим наши таблицы\n",
    "\n",
    "assert(df_equal(x_test[x_test_ref.columns], x_test_ref))\n",
    "assert(df_equal(x_train[x_train_ref.columns], x_train_ref))\n",
    "assert(df_equal(y_train, y_train_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params_all = list(x_train.columns)\n",
    "\n",
    "params_orig = [\n",
    "'maxPlayerLevel',\n",
    "'numberOfAttemptedLevels',\n",
    "'attemptsOnTheHighestLevel',\n",
    "'totalNumOfAttempts',\n",
    "'averageNumOfTurnsPerCompletedLevel',\n",
    "'numberOfBoostersUsed',\n",
    "'fractionOfUsefullBoosters',\n",
    "'totalScore',\n",
    "'totalBonusScore',\n",
    "'totalStarsCount',\n",
    "'numberOfDaysActuallyPlayed',\n",
    "]\n",
    "     \n",
    "params_mod = [\n",
    "#'attemptsOnTheHighestLevel_dvd',\n",
    "'attemptsOnTheHighestLevel_ln',\n",
    "'attemptsPerDay',\n",
    "'averageNumOfTurnsPerCompletedLevel_dvd',\n",
    "'maxPlayerLevel_ln',\n",
    "'numberOfAttemptedLevels_dvd',\n",
    "#'numberOfBoostersUsed_dvd',\n",
    "'numberOfBoostersUsed_dvd_ln',\n",
    "#'numberOfBoostersUsed_ln',\n",
    "'numberOfDaysActuallyPlayed_ln',\n",
    "'totalBonusScore_dvd',\n",
    "'totalNumOfAttempts_ln',\n",
    "#'totalScore_ln',\n",
    "'totalScore_ln_dvd',\n",
    "#'totalStarsCount_dvd',\n",
    "'totalStarsCount_dvd_ln',\n",
    "#'totalStarsCount_ln',\n",
    "]\n",
    "\n",
    "params_bool = [\n",
    "'allAttemptsOnTheHighestLevel',\n",
    "'attLevelsMoreThanMaxLevel',\n",
    "'doReturnOnLowerLevels',\n",
    "'zeroTotalScore',\n",
    "'zeroTurnsPerCompletedLevel'\n",
    "]\n",
    "\n",
    "params_orig_norm = [\n",
    "'attemptsOnTheHighestLevel_norm',\n",
    "'attemptsPerDay_norm',\n",
    "#'numberOfBoostersUsed_norm',\n",
    "#'maxPlayerLevel_norm',\n",
    "'fractionOfUsefullBoosters_norm',\n",
    "#'averageNumOfTurnsPerCompletedLevel_norm',\n",
    "'numberOfDaysActuallyPlayed_norm',\n",
    "'totalNumOfAttempts_norm',\n",
    "'totalBonusScore_norm',\n",
    "'totalScore_norm',\n",
    "'totalStarsCount_norm',\n",
    "]\n",
    "\n",
    "params_mod_norm = [\n",
    "'attemptsOnTheHighestLevel_dvd_norm',\n",
    "#'attemptsOnTheHighestLevel_ln_norm',\n",
    "'averageNumOfTurnsPerCompletedLevel_dvd_norm',\n",
    "'maxPlayerLevel_ln_norm',\n",
    "'numberOfAttemptedLevels_dvd_norm',\n",
    "#'numberOfAttemptedLevels_norm',\n",
    "'numberOfBoostersUsed_dvd_ln_norm',\n",
    "#'numberOfBoostersUsed_dvd_norm',\n",
    "#'numberOfBoostersUsed_ln_norm',\n",
    "'numberOfDaysActuallyPlayed_ln_norm',\n",
    "'totalBonusScore_dvd_norm',\n",
    "'totalNumOfAttempts_ln_norm',\n",
    "'totalScore_ln_dvd_norm',\n",
    "#'totalScore_ln_norm',\n",
    "'totalStarsCount_dvd_ln_norm',\n",
    "#'totalStarsCount_dvd_norm',\n",
    "'totalStarsCount_ln_norm',\n",
    "]\n",
    "\n",
    "params_rf_sorted = [\n",
    "'numberOfDaysActuallyPlayed_ln',\n",
    "'totalNumOfAttempts',\n",
    "'totalBonusScore_dvd_norm',\n",
    "'averageNumOfTurnsPerCompletedLevel',\n",
    "'totalScore_ln_dvd',\n",
    "'averageNumOfTurnsPerCompletedLevel_dvd',\n",
    "'maxPlayerLevel_ln_norm',\n",
    "'attemptsOnTheHighestLevel_dvd_norm',\n",
    "'totalStarsCount_dvd_norm',\n",
    "'attemptsPerDay_norm',\n",
    "'numberOfBoostersUsed_dvd_norm',\n",
    "'fractionOfUsefullBoosters_norm',\n",
    "'numberOfAttemptedLevels_dvd_norm',\n",
    "'doReturnOnLowerLevels',\n",
    "'attLevelsMoreThanMaxLevel',\n",
    "'allAttemptsOnTheHighestLevel',\n",
    "'zeroTotalScore',\n",
    "'zeroTurnsPerCompletedLevel'\n",
    "]\n",
    "\n",
    "params_gb_sorted = [\n",
    "'averageNumOfTurnsPerCompletedLevel_norm',\n",
    "'numberOfDaysActuallyPlayed_norm',\n",
    "'attemptsPerDay_norm',\n",
    "'totalNumOfAttempts',\n",
    "'numberOfDaysActuallyPlayed',\n",
    "'attemptsOnTheHighestLevel_dvd',\n",
    "'numberOfBoostersUsed_dvd',\n",
    "'totalScore_ln_dvd',\n",
    "'fractionOfUsefullBoosters',\n",
    "'maxPlayerLevel_norm',\n",
    "'numberOfAttemptedLevels_dvd',\n",
    "'totalBonusScore_norm',\n",
    "'totalStarsCount_dvd_norm'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#params = params_orig + [\"doReturnOnLowerLevels\"]\n",
    "#params = params_mod_norm + params_bool\n",
    "#params = params_bool\n",
    "#params = params_bool + params_orig\n",
    "#params = params_all\n",
    "params = params_rf_sorted\n",
    "#paramslist(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RF = ML(ensemble.RandomForestClassifier(n_estimators=200, max_features=10, random_state=42),\n",
    "        params_rf_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.11315754714695353, 0.46972305554348548)\n"
     ]
    }
   ],
   "source": [
    "print(RF.log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GB = ML(ensemble.GradientBoostingClassifier(n_estimators=100, random_state=42), \n",
    "        params_gb_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.36954064074598247, 0.37820870447373117)\n"
     ]
    }
   ],
   "source": [
    "print(GB.log_loss)\n",
    "#pprint(GB.imp)\n",
    "#for i in GB.imp: print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "XGB = ML(xgb.XGBClassifier(),\n",
    "        params_gb_sorted,\n",
    "        split=[\"zeroTotalScore\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.36714156339229764, 0.37983107549635881)\n"
     ]
    }
   ],
   "source": [
    "print(XGB.log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reload(tflow)\n",
    "TF = ML(tflow.nnetwork(),\n",
    "        params_mod_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.39035322911592912, 0.379951024531613)\n"
     ]
    }
   ],
   "source": [
    "print(TF.log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# как перебирать по набору параметров\n",
    "\n",
    "results = []\n",
    "for i, params in enumerate([params_orig, \n",
    "                            params_mod, \n",
    "                            params_bool, \n",
    "                            params_orig_norm, \n",
    "                            params_mod_norm, \n",
    "                            params_rf_sorted, \n",
    "                            params_all,\n",
    "                            params_gb_sorted,\n",
    "                           ]):\n",
    "    try:\n",
    "        XGB = ML(xgb.XGBClassifier(max_depth=6), params)\n",
    "        results.append((XGB.log_loss[1], i))\n",
    "    except FitError:\n",
    "        pass\n",
    "results.sort()\n",
    "pprint(results[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# как перебирать по сплитам\n",
    "\n",
    "params_split = [\n",
    "'allAttemptsOnTheHighestLevel',\n",
    "'attLevelsMoreThanMaxLevel',\n",
    "'doReturnOnLowerLevels',\n",
    "'zeroTotalScore',\n",
    "'zeroTurnsPerCompletedLevel'\n",
    "]\n",
    "\n",
    "params_split_combs = all_perm(params_split)\n",
    "results = []\n",
    "for split in params_split_combs:\n",
    "    try:\n",
    "        XGB = ML(xgb.XGBClassifier(max_depth=6), params_mod, split=split)\n",
    "        results.append((XGB.log_loss[1], split))\n",
    "    except FitError:\n",
    "        pass\n",
    "results.sort()\n",
    "pprint(results[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# как перебирать по параметрам\n",
    "\n",
    "depths = [4, 5, 6]\n",
    "learning_rates = [0.08, 0.1, 0.12]\n",
    "estimators = [55, 70, 85, 100]\n",
    "results = []\n",
    "for depth, learning_rate, n_estimators in itertools.product(depths, learning_rates, estimators):\n",
    "    try:\n",
    "        XGB = ML(xgb.XGBClassifier(max_depth=depth, \n",
    "                                   learning_rate=learning_rate, \n",
    "                                   n_estimators=n_estimators), params_mod, split=[])\n",
    "        results.append((XGB.log_loss[1], XGB.log_loss[0], depth, learning_rate, n_estimators))\n",
    "    except FitError:\n",
    "        pass\n",
    "results.sort()\n",
    "pprint(results[:5])\n",
    "#max_depth=3, learning_rate=0.1, n_estimators=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "XGB = ML(xgb.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=70), params_mod + params_bool, split=[])\n",
    "#print(XGB.log_loss)\n",
    "#XGB.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TF.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, grid_search, datasets\n",
    "parameters = {'max_depth':(2, 3, 4), 'learning_rate':[0.105, 0.11, 0.115]}\n",
    "svr = xgb.XGBClassifier()\n",
    "clf = grid_search.GridSearchCV(XGB.model, parameters)\n",
    "clf.fit(x_train[params_mod], list(y_train.returned))\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(XGB.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
