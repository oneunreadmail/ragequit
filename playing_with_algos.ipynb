{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import itertools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import model_selection, ensemble\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FitError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# функция на работу с уже полученными вероятностями\n",
    "# пока не в деле, но я думаю, что их хитрым образом можно как-то потрогать и улучшить логлосс\n",
    "def expand(array, k=0):\n",
    "    k = np.array(k).reshape((-1, 1))\n",
    "    new = (array - 0.5) * (1 + k) + 0.5\n",
    "    return [i for i in new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# делает из листа листов листов листов один плоский лист\n",
    "def flatten(seq, container=None):\n",
    "    if container is None:\n",
    "        container = []\n",
    "    for s in seq:\n",
    "        if hasattr(s,'__iter__'):\n",
    "            flatten(s,container)\n",
    "        else:\n",
    "            container.append(s)\n",
    "    return container\n",
    "\n",
    "# считает элементы в листе листов листов листов\n",
    "# тупо чтобы проверить, что мы не потеряли строчки где-то\n",
    "def flat_len(seq):\n",
    "    return len(flatten(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# выдает лист из всех комбинаций элементов входого листа\n",
    "# all_perm([\"0\", \"1\", \"2\"])\n",
    "# (['2', '1', '0'], ['1', '0'], ['2', '0'], ['0'], ['2', '1'], ['1'], ['2'], [])\n",
    "def all_perm(list):\n",
    "    if len(list) == 1:\n",
    "        return [list[0]],[]\n",
    "    else:\n",
    "        ap = all_perm(list[1:])\n",
    "        #print(ap)\n",
    "        #print(tuple(lst + [list[0]] for lst in ap))\n",
    "        #print(tuple(lst for lst in ap))\n",
    "        return (tuple(lst + [list[0]] for lst in ap)) + (tuple(lst for lst in ap))\n",
    "\n",
    "#all_perm([\"0\", \"1\", \"2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# сплиттит датафреймы по листу из n индексов\n",
    "# выдает тупл из 2**n элементов, каждый - лист из номеров строк\n",
    "def df_split(df, indices = []):\n",
    "    if len(indices) > 1:\n",
    "        left =  df_split(df[df[indices[0]] == 0], indices[1:])\n",
    "        right = df_split(df[df[indices[0]] == 1], indices[1:])\n",
    "        assert(len(df.index) == flat_len(left) + flat_len(right))\n",
    "        return left + right\n",
    "    elif len(indices) == 1:\n",
    "        left = list(df[df[indices[0]] == 0].index)\n",
    "        right = list(df[df[indices[0]] == 1].index)\n",
    "        #if not left: return right,\n",
    "        #elif not right: return left,\n",
    "        #else: \n",
    "        return left, right\n",
    "    else:\n",
    "        return list(df.index),\n",
    "\n",
    "#f = x_train[[\"maxPlayerLevel\", \"attLevelsMoreThanMaxLevel\", \"doReturnOnLowerLevels\", \"allAttemptsOnTheHighestLevel\"]][30:50]\n",
    "#f = x_train[[\"maxPlayerLevel\", \"true\"]][30:35]\n",
    "#f.loc[30:35]\n",
    "#g = df_split(f)\n",
    "#g = df_split(f, [])\n",
    "#g = df_split(f, [\"attLevelsMoreThanMaxLevel\"])\n",
    "#g = df_split(f, [\"attLevelsMoreThanMaxLevel\", \"doReturnOnLowerLevels\", \"allAttemptsOnTheHighestLevel\"])\n",
    "#for i in g: print(i)\n",
    "#f.loc[g[1]]\n",
    "\n",
    "#df_split(f, [\"attLevelsMoreThanMaxLevel\", \"doReturnOnLowerLevels\", \"allAttemptsOnTheHighestLevel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# класс с нашими моделями\n",
    "# сразу при вызове забирает данные из глобальных x_test, t_train и y_train\n",
    "# учитывает только колонки из листа params (локальный, обязательный)\n",
    "# бьет их на части из листа split (локальный, необязательный)\n",
    "# сразу все фитует и даже считает y_test\n",
    "\n",
    "class ML():\n",
    "    model = None\n",
    "    errors = None\n",
    "    log_loss = None\n",
    "    data = []\n",
    "    test_out = None\n",
    "    \n",
    "    \n",
    "    def __init__(self, model, params, split=[]):\n",
    "        self.params = params\n",
    "        self.model = model\n",
    "        self.split = split\n",
    "        self.form()\n",
    "        self.fit()\n",
    "\n",
    "    \n",
    "    def form(self):\n",
    "        self.train_indices = df_split(x_train, self.split)\n",
    "        self.test_indices = df_split(x_test, self.split)\n",
    "        \n",
    "        # проверяем по всем кускам\n",
    "        # если куска нет в трейне, то обучить тест не получится\n",
    "        for i in self.train_indices:\n",
    "            if (i == []) and (self.test_indices != []):\n",
    "                raise FitError('CANT FIT DIS: some empty train stuff but not test one, u know')\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        y1v, y2v, y1p, y2p, y1r, y2r = [], [], [], [], [], []\n",
    "        out, xtr = [], []\n",
    "        for i, index in enumerate(self.train_indices):\n",
    "            if len(index) == 0: continue\n",
    "            x_train_part = x_train[params].loc[index]\n",
    "            y_train_part = list(y_train.returned.loc[index])\n",
    "            x1, x2, y1, y2 = train_test_split(x_train_part, y_train_part, test_size = 0.3, random_state=42)\n",
    "            \n",
    "            try:\n",
    "                self.model.fit(x1, y1)\n",
    "            except ValueError:\n",
    "                raise FitError(\"CANT FIR DIS: looks like it needs more rows or something else\")\n",
    "            \n",
    "            # values\n",
    "            y1v.append(self.model.predict(x1))\n",
    "            y2v.append(self.model.predict(x2))\n",
    "            # probabilities\n",
    "            y1p.append(self.model.predict_proba(x1))\n",
    "            y2p.append(self.model.predict_proba(x2))\n",
    "            # true values\n",
    "            y1r.append(np.array(y1))\n",
    "            y2r.append(np.array(y2))\n",
    "            # applying model to test values\n",
    "            if len(self.test_indices[i]) != 0:\n",
    "                # если есть кусок теста, отфитуем его\n",
    "                x_test_part = x_test[params].loc[self.test_indices[i]]\n",
    "                out.append(self.model.predict_proba(x_test_part)[:, 1])\n",
    "        \n",
    "        # будем все собирать\n",
    "        # следующие шесть можно собирать не по порядку, какая в жопу разница, логлоссу все равно\n",
    "        y1v = np.concatenate(y1v)\n",
    "        y2v = np.concatenate(y2v)\n",
    "        y1p = np.concatenate(y1p)\n",
    "        y2p = np.concatenate(y2p)\n",
    "        y1r = np.concatenate(y1r)\n",
    "        y2r = np.concatenate(y2r)\n",
    "\n",
    "        # а вот для ответов на тесты следим за рукой\n",
    "        # пока что out это лист из массивов с предсказаниями для разных кусков\n",
    "        # теперь сделаем из него лист из двумерных массивов вида \"номера корректных строк\", \"предсказания\"\n",
    "        out1 = [np.array([self.test_indices[_], __]) for _, __ in enumerate(out)]\n",
    "        # соберем один двумерный массив из этой кучи\n",
    "        # теперь там куча строк вида [1, 0.44] [2, 0.62], но номера перепутаны, потому что мы резали\n",
    "        out2 = np.concatenate(out1, axis=1).transpose()\n",
    "        # отсортируем\n",
    "        out3 = out2[np.argsort(out2[:, 0])]\n",
    "        # запишем предсказания для теста и вот мы молодцы\n",
    "        self.test_out = out3[:, 1]\n",
    "        \n",
    "        self.errors = np.mean(y1r != y1v), np.mean(y2r != y2v)\n",
    "        self.log_loss = log_loss(y1r, y1p, eps=1e-15), log_loss(y2r, y2p, eps=1e-15)\n",
    "                              \n",
    "        #importances = self.model.feature_importances_\n",
    "    \n",
    "    def dump(self):\n",
    "        #a = self.model.predict_proba(data)\n",
    "        np.savetxt(\"data/y_test.csv\", self.test_out, fmt='%10.5f')\n",
    "\n",
    "#XGB = ML(xgb.XGBClassifier(), params, split=params_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xytte = pd.read_csv(\"data/xytte.csv\")\n",
    "xytte[\"true\"] = 1\n",
    "\n",
    "####################### ВНИМАНИЕ #######################\n",
    "# здесь трейн это которые в задаче трейн, с ответами\n",
    "# тест без ответов\n",
    "# но дальше в алгоритмах трейн будет разбиваться на два подсета\n",
    "# которые будут у меня называться x1-y1 и x2-y2, а по смыслу как раз трейн и тест\n",
    "# надо не перепутать\n",
    "\n",
    "x_train = xytte[xytte.returned == xytte.returned].reset_index(drop=True).drop(\"returned\", axis=1)\n",
    "y_train = xytte[xytte.returned == xytte.returned].reset_index(drop=True)[[\"returned\"]]\n",
    "x_test  = xytte[xytte.returned != xytte.returned].reset_index(drop=True).drop(\"returned\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# проверяем, что нигде не налажали\n",
    "# читаем из файла оригинальные данные\n",
    "\n",
    "x_test_ref = pd.read_csv(\"data/x_test.csv\", sep=\";\", dtype=np.float32)\n",
    "x_train_ref = pd.read_csv(\"data/x_train.csv\", sep=\";\", dtype=np.float32)\n",
    "y_train_ref = pd.read_csv(\"data/y_train.csv\", sep=\";\", dtype=np.float32, header=None, names=[\"returned\"])\n",
    "\n",
    "# заводим функцию на сравнение таблиц\n",
    "\n",
    "def df_equal(df1, df2):\n",
    "    if list(df1.columns) != list(df2.columns): \n",
    "        print(\"Mismatch columns:\")\n",
    "        print(\"df1: \", list(df1.columns))\n",
    "        print(\"df2: \", list(df2.columns))\n",
    "        return False\n",
    "    if list(df1.index) != list(df2.index): \n",
    "        print(\"Mismatch index:\")\n",
    "        print(\"df1: \", list(df1.columns)[0], \":\", list(df1.columns)[-1])\n",
    "        print(\"df2: \", list(df2.columns)[0], \":\", list(df2.columns)[-1])\n",
    "        return False\n",
    "    \n",
    "    ne_stacked = (df1 != df2).stack()\n",
    "    changed = ne_stacked[ne_stacked]\n",
    "    changed.index.names = ['id', 'col']\n",
    "\n",
    "    difference_locations = np.where(df1 != df2)\n",
    "    changed_from = df1.values[difference_locations]\n",
    "    changed_to = df2.values[difference_locations]\n",
    "    diff = pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)\n",
    "    return not diff[diff[\"from\"] - diff[\"to\"] > 0.000001].count()[0]\n",
    "\n",
    "# ассертим наши таблицы\n",
    "\n",
    "assert(df_equal(x_test[x_test_ref.columns], x_test_ref))\n",
    "assert(df_equal(x_train[x_train_ref.columns], x_train_ref))\n",
    "assert(df_equal(y_train, y_train_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params_all = list(x_train.columns)\n",
    "\n",
    "params_orig = [\n",
    "'maxPlayerLevel',\n",
    "'numberOfAttemptedLevels',\n",
    "'attemptsOnTheHighestLevel',\n",
    "'totalNumOfAttempts',\n",
    "'averageNumOfTurnsPerCompletedLevel',\n",
    "'numberOfBoostersUsed',\n",
    "'fractionOfUsefullBoosters',\n",
    "'totalScore',\n",
    "'totalBonusScore',\n",
    "'totalStarsCount',\n",
    "'numberOfDaysActuallyPlayed',\n",
    "]\n",
    "     \n",
    "params_mod = [\n",
    "#'attemptsOnTheHighestLevel_dvd',\n",
    "'attemptsOnTheHighestLevel_ln',\n",
    "'attemptsPerDay',\n",
    "'averageNumOfTurnsPerCompletedLevel_dvd',\n",
    "'maxPlayerLevel_ln',\n",
    "'numberOfAttemptedLevels_dvd',\n",
    "#'numberOfBoostersUsed_dvd',\n",
    "'numberOfBoostersUsed_dvd_ln',\n",
    "#'numberOfBoostersUsed_ln',\n",
    "'numberOfDaysActuallyPlayed_ln',\n",
    "'totalBonusScore_dvd',\n",
    "'totalNumOfAttempts_ln',\n",
    "#'totalScore_ln',\n",
    "'totalScore_ln_dvd',\n",
    "#'totalStarsCount_dvd',\n",
    "'totalStarsCount_dvd_ln',\n",
    "#'totalStarsCount_ln',\n",
    "]\n",
    "\n",
    "params_bool = [\n",
    "'allAttemptsOnTheHighestLevel',\n",
    "'attLevelsMoreThanMaxLevel',\n",
    "'doReturnOnLowerLevels',\n",
    "'zeroTotalScore',\n",
    "'zeroTurnsPerCompletedLevel'\n",
    "]\n",
    "\n",
    "params_orig_norm = [\n",
    "'attemptsOnTheHighestLevel_norm',\n",
    "'attemptsPerDay_norm',\n",
    "#'numberOfBoostersUsed_norm',\n",
    "#'maxPlayerLevel_norm',\n",
    "'fractionOfUsefullBoosters_norm',\n",
    "#'averageNumOfTurnsPerCompletedLevel_norm',\n",
    "'numberOfDaysActuallyPlayed_norm',\n",
    "'totalNumOfAttempts_norm',\n",
    "'totalBonusScore_norm',\n",
    "'totalScore_norm',\n",
    "'totalStarsCount_norm',\n",
    "]\n",
    "\n",
    "params_mod_norm = [\n",
    "'attemptsOnTheHighestLevel_dvd_norm',\n",
    "#'attemptsOnTheHighestLevel_ln_norm',\n",
    "'averageNumOfTurnsPerCompletedLevel_dvd_norm',\n",
    "'maxPlayerLevel_ln_norm',\n",
    "'numberOfAttemptedLevels_dvd_norm',\n",
    "#'numberOfAttemptedLevels_norm',\n",
    "'numberOfBoostersUsed_dvd_ln_norm',\n",
    "#'numberOfBoostersUsed_dvd_norm',\n",
    "#'numberOfBoostersUsed_ln_norm',\n",
    "'numberOfDaysActuallyPlayed_ln_norm',\n",
    "'totalBonusScore_dvd_norm',\n",
    "'totalNumOfAttempts_ln_norm',\n",
    "'totalScore_ln_dvd_norm',\n",
    "#'totalScore_ln_norm',\n",
    "'totalStarsCount_dvd_ln_norm',\n",
    "#'totalStarsCount_dvd_norm',\n",
    "'totalStarsCount_ln_norm',\n",
    "]\n",
    "\n",
    "params_rf_sorted = [\n",
    "'numberOfDaysActuallyPlayed_ln',\n",
    "'totalNumOfAttempts',\n",
    "'totalBonusScore_dvd_norm',\n",
    "'averageNumOfTurnsPerCompletedLevel',\n",
    "'totalScore_ln_dvd',\n",
    "'averageNumOfTurnsPerCompletedLevel_dvd',\n",
    "'maxPlayerLevel_ln_norm',\n",
    "'attemptsOnTheHighestLevel_dvd_norm',\n",
    "'totalStarsCount_dvd_norm',\n",
    "'attemptsPerDay_norm',\n",
    "'numberOfBoostersUsed_dvd_norm',\n",
    "'fractionOfUsefullBoosters_norm',\n",
    "'numberOfAttemptedLevels_dvd_norm',\n",
    "'doReturnOnLowerLevels',\n",
    "'attLevelsMoreThanMaxLevel',\n",
    "'allAttemptsOnTheHighestLevel',\n",
    "'zeroTotalScore',\n",
    "'zeroTurnsPerCompletedLevel'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#params = params_orig + [\"doReturnOnLowerLevels\"]\n",
    "#params = params_mod_norm + params_bool\n",
    "#params = params_bool\n",
    "#params = params_bool + params_orig\n",
    "#params = params_all\n",
    "params = params_rf_sorted\n",
    "#paramslist(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x_train_feed = x_train[params]\n",
    "x_test_feed = x_test[params]\n",
    "y_train_feed = list(y_train.returned)\n",
    "#y_test_feed = list(y_test)\n",
    "\n",
    "x1, x2, y1, y2 = train_test_split(x_train_feed, y_train_feed, test_size = 0.3, random_state=42)\n",
    "\n",
    "assert(set(list(x_train_feed.columns)) == set(list(x_test_feed.columns)))\n",
    "assert(set(list(x1.columns)) == set(list(x2.columns)))\n",
    "assert(type(y1) == list)\n",
    "assert(type(y2) == list)\n",
    "assert(len(y1) == len(x1.index))\n",
    "assert(len(y2) == len(x2.index))\n",
    "\n",
    "#list(x1.columns), list(x2.columns) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RF = ML(ensemble.RandomForestClassifier(n_estimators=200, max_features=10, random_state=42),\n",
    "        params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.11315754714695353, 0.46972305554348548)\n"
     ]
    }
   ],
   "source": [
    "print(RF.log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GB = ML(ensemble.GradientBoostingClassifier(n_estimators=100, random_state=42), \n",
    "        params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.36909800135902776, 0.37801783938038297)\n"
     ]
    }
   ],
   "source": [
    "print(GB.log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XGB = ML(xgb.XGBClassifier(),\n",
    "        params,\n",
    "        split=[\"zeroTotalScore\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.36585039983917783, 0.38018562523907151)\n"
     ]
    }
   ],
   "source": [
    "print(XGB.log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.37755557326602412, 1),\n",
      " (0.37762009812220909, 6),\n",
      " (0.37766712194851082, 4),\n",
      " (0.37783503698610299, 0),\n",
      " (0.37797661614552125, 5)]\n"
     ]
    }
   ],
   "source": [
    "# как перебирать по набору параметров\n",
    "\n",
    "results = []\n",
    "for i, params in enumerate([params_orig, \n",
    "                            params_mod, \n",
    "                            params_bool, \n",
    "                            params_orig_norm, \n",
    "                            params_mod_norm, \n",
    "                            params_rf_sorted, \n",
    "                            params_all]):\n",
    "    try:\n",
    "        XGB = ML(xgb.XGBClassifier(), params)\n",
    "        results.append((XGB.log_loss[1], i))\n",
    "    except FitError:\n",
    "        pass\n",
    "results.sort()\n",
    "pprint(results[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.37762009812220909, []),\n",
      " (0.38037075118176589, ['zeroTotalScore']),\n",
      " (0.38037075118176589, ['zeroTurnsPerCompletedLevel']),\n",
      " (0.38089123439965755, ['attLevelsMoreThanMaxLevel']),\n",
      " (0.3815153109908308, ['doReturnOnLowerLevels'])]\n"
     ]
    }
   ],
   "source": [
    "# как перебирать по сплитам\n",
    "\n",
    "params_split = [\n",
    "'allAttemptsOnTheHighestLevel',\n",
    "'attLevelsMoreThanMaxLevel',\n",
    "'doReturnOnLowerLevels',\n",
    "'zeroTotalScore',\n",
    "'zeroTurnsPerCompletedLevel'\n",
    "]\n",
    "\n",
    "params_split_combs = all_perm(params_split)\n",
    "results = []\n",
    "for split in params_split_combs:\n",
    "    try:\n",
    "        XGB = ML(xgb.XGBClassifier(), params_mod, split=split)\n",
    "        results.append((XGB.log_loss[1], split))\n",
    "    except FitError:\n",
    "        pass\n",
    "results.sort()\n",
    "pprint(results[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3767954628409223, 4, 0.05, 200),\n",
      " (0.37692667287924758, 2, 0.15, 200),\n",
      " (0.37694352674182602, 2, 0.1, 200),\n",
      " (0.37698885952950473, 2, 0.15, 100),\n",
      " (0.3771754438168019, 2, 0.05, 300)]\n"
     ]
    }
   ],
   "source": [
    "# как перебирать по параметрам\n",
    "\n",
    "depths = [2, 3, 4]\n",
    "learning_rates = [0.05, 0.1, 0.15]\n",
    "estimators = [100, 200, 300]\n",
    "results = []\n",
    "for depth, learning_rate, n_estimators in itertools.product(depths, learning_rates, estimators):\n",
    "    try:\n",
    "        XGB = ML(xgb.XGBClassifier(max_depth=depth, \n",
    "                                   learning_rate=learning_rate, \n",
    "                                   n_estimators=n_estimators), params_mod, split=[])\n",
    "        results.append((XGB.log_loss[1], XGB.log_loss[0], depth, learning_rate, n_estimators))\n",
    "    except FitError:\n",
    "        pass\n",
    "results.sort()\n",
    "pprint(results[:5])\n",
    "#max_depth=3, learning_rate=0.1, n_estimators=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.35802886187307364, 0.3767954628409223)\n"
     ]
    }
   ],
   "source": [
    "XGB = ML(xgb.XGBClassifier(max_depth=4, learning_rate=0.05, n_estimators=200), params_mod, split=[])\n",
    "print(XGB.log_loss)\n",
    "XGB.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
