{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import model_selection, ensemble\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "def random_forest(x1, x2, y1, y2, n_estimators=20, max_features=5):\n",
    "    rf = ensemble.RandomForestClassifier(n_estimators=n_estimators, max_features=max_features)\n",
    "    rf.fit(x1, y1)\n",
    "    err_train = np.mean(y1 != rf.predict(x1))\n",
    "    err_test  = np.mean(y2  != rf.predict(x2))\n",
    "    return err_train, err_test\n",
    "\n",
    "# GRAD BOOST\n",
    "\n",
    "def grad_boost(x1, x2, y1, y2, n_estimators=20):\n",
    "    gbt = ensemble.GradientBoostingClassifier(n_estimators=n_estimators)\n",
    "    gbt.fit(x1, y1)\n",
    "    err_train = np.mean(y1 != gbt.predict(x1))\n",
    "    err_test = np.mean(y2 != gbt.predict(x2))\n",
    "    return err_train, err_test\n",
    "\n",
    "# XGBOOST\n",
    "\n",
    "def xgboost(x1, x2, y1, y2):\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.fit(x1, y1)\n",
    "    err_train = np.mean(y1 != model.predict(x1))\n",
    "    err_test = np.mean(y2 != model.predict(x2))\n",
    "    return err_train, err_test\n",
    "\n",
    "class ML():\n",
    "    model = None\n",
    "    errors = None\n",
    "    log_loss = None\n",
    "    \n",
    "    def __init__(self, x1, x2, y1, y2, model):\n",
    "        self.x1, self.y1, self.x2, self.y2 = x1, y1, x2, y2\n",
    "        self.model = model\n",
    "        self.fit()\n",
    "    \n",
    "    def fit(self):\n",
    "        self.model.fit(self.x1, self.y1)\n",
    "        y1p, y2p = self.model.predict(self.x1), self.model.predict(self.x2)\n",
    "        self.errors = np.mean(self.y1 != y1p), np.mean(self.y2 != y2p)\n",
    "        self.log_loss = log_loss(self.y1, y1p, eps=1e-15), log_loss(self.y2, y2p, eps=1e-15)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self.model.predict(data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xytte = pd.read_csv(\"data/xytte.csv\")\n",
    "\n",
    "####################### ВНИМАНИЕ #######################\n",
    "# здесь трейн это которые в задаче трейн, с ответами\n",
    "# тест без ответов\n",
    "# но дальше в алгоритмах трейн будет разбиваться на два подсета\n",
    "# которые будут у меня называться x1-y1 и x2-y2, а по смыслу как раз трейн и тест\n",
    "# надо не перепутать\n",
    "\n",
    "x_train = xytte[xytte.returned == xytte.returned].reset_index(drop=True).drop(\"returned\", axis=1)\n",
    "y_train = xytte[xytte.returned == xytte.returned].reset_index(drop=True)[[\"returned\"]]\n",
    "x_test  = xytte[xytte.returned != xytte.returned].reset_index(drop=True).drop(\"returned\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# проверяем, что нигде не налажали\n",
    "# читаем из файла оригинальные данные\n",
    "\n",
    "x_test_ref = pd.read_csv(\"data/x_test.csv\", sep=\";\", dtype=np.float32)\n",
    "x_train_ref = pd.read_csv(\"data/x_train.csv\", sep=\";\", dtype=np.float32)\n",
    "y_train_ref = pd.read_csv(\"data/y_train.csv\", sep=\";\", dtype=np.float32, header=None, names=[\"returned\"])\n",
    "\n",
    "# заводим функцию на сравнение таблиц\n",
    "\n",
    "def df_equal(df1, df2):\n",
    "    if list(df1.columns) != list(df2.columns): \n",
    "        print(\"Mismatch columns:\")\n",
    "        print(\"df1: \", list(df1.columns))\n",
    "        print(\"df2: \", list(df2.columns))\n",
    "        return False\n",
    "    if list(df1.index) != list(df2.index): \n",
    "        print(\"Mismatch index:\")\n",
    "        print(\"df1: \", list(df1.columns)[0], \":\", list(df1.columns)[-1])\n",
    "        print(\"df2: \", list(df2.columns)[0], \":\", list(df2.columns)[-1])\n",
    "        return False\n",
    "    \n",
    "    ne_stacked = (df1 != df2).stack()\n",
    "    changed = ne_stacked[ne_stacked]\n",
    "    changed.index.names = ['id', 'col']\n",
    "\n",
    "    difference_locations = np.where(df1 != df2)\n",
    "    changed_from = df1.values[difference_locations]\n",
    "    changed_to = df2.values[difference_locations]\n",
    "    diff = pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)\n",
    "    return not diff[diff[\"from\"] - diff[\"to\"] > 0.000001].count()[0]\n",
    "\n",
    "# ассертим наши таблицы\n",
    "\n",
    "assert(df_equal(x_test[x_test_ref.columns], x_test_ref))\n",
    "assert(df_equal(x_train[x_train_ref.columns], x_train_ref))\n",
    "assert(df_equal(y_train, y_train_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params_all = list(x_train.columns)\n",
    "\n",
    "params_orig = [\n",
    "'maxPlayerLevel',\n",
    "'numberOfAttemptedLevels',\n",
    "'attemptsOnTheHighestLevel',\n",
    "'totalNumOfAttempts',\n",
    "'averageNumOfTurnsPerCompletedLevel',\n",
    "'numberOfBoostersUsed',\n",
    "'fractionOfUsefullBoosters',\n",
    "'totalScore',\n",
    "'totalBonusScore',\n",
    "'totalStarsCount',\n",
    "'numberOfDaysActuallyPlayed',\n",
    "]\n",
    "     \n",
    "params_mod = [\n",
    "#'attemptsOnTheHighestLevel_dvd',\n",
    "'attemptsOnTheHighestLevel_ln',\n",
    "'attemptsPerDay',\n",
    "'averageNumOfTurnsPerCompletedLevel_dvd',\n",
    "'maxPlayerLevel_ln',\n",
    "'numberOfAttemptedLevels_dvd',\n",
    "#'numberOfBoostersUsed_dvd',\n",
    "'numberOfBoostersUsed_dvd_ln',\n",
    "#'numberOfBoostersUsed_ln',\n",
    "'numberOfDaysActuallyPlayed_ln',\n",
    "'totalBonusScore_dvd',\n",
    "'totalNumOfAttempts_ln',\n",
    "#'totalScore_ln',\n",
    "'totalScore_ln_dvd',\n",
    "#'totalStarsCount_dvd',\n",
    "'totalStarsCount_dvd_ln',\n",
    "#'totalStarsCount_ln',\n",
    "]\n",
    "\n",
    "params_bool = [\n",
    "'allAttemptsOnTheHighestLevel',\n",
    "'attLevelsMoreThanMaxLevel',\n",
    "'doReturnOnLowerLevels',\n",
    "'zeroTotalScore',\n",
    "'zeroTurnsPerCompletedLevel'\n",
    "]\n",
    "\n",
    "params_orig_norm = [\n",
    "'attemptsOnTheHighestLevel_norm',\n",
    "'attemptsPerDay_norm',\n",
    "#'numberOfBoostersUsed_norm',\n",
    "#'maxPlayerLevel_norm',\n",
    "'fractionOfUsefullBoosters_norm',\n",
    "#'averageNumOfTurnsPerCompletedLevel_norm',\n",
    "'numberOfDaysActuallyPlayed_norm',\n",
    "'totalNumOfAttempts_norm',\n",
    "'totalBonusScore_norm',\n",
    "'totalScore_norm',\n",
    "'totalStarsCount_norm',\n",
    "]\n",
    "\n",
    "params_mod_norm = [\n",
    "'attemptsOnTheHighestLevel_dvd_norm',\n",
    "#'attemptsOnTheHighestLevel_ln_norm',\n",
    "'averageNumOfTurnsPerCompletedLevel_dvd_norm',\n",
    "'maxPlayerLevel_ln_norm',\n",
    "'numberOfAttemptedLevels_dvd_norm',\n",
    "#'numberOfAttemptedLevels_norm',\n",
    "'numberOfBoostersUsed_dvd_ln_norm',\n",
    "#'numberOfBoostersUsed_dvd_norm',\n",
    "#'numberOfBoostersUsed_ln_norm',\n",
    "'numberOfDaysActuallyPlayed_ln_norm',\n",
    "'totalBonusScore_dvd_norm',\n",
    "'totalNumOfAttempts_ln_norm',\n",
    "'totalScore_ln_dvd_norm',\n",
    "#'totalScore_ln_norm',\n",
    "'totalStarsCount_dvd_ln_norm',\n",
    "#'totalStarsCount_dvd_norm',\n",
    "'totalStarsCount_ln_norm',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#params = params_orig + [\"doReturnOnLowerLevels\"]\n",
    "#params = params_mod + params_bool\n",
    "#params = params_bool\n",
    "params = params_all\n",
    "#params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train_feed = x_train[params]\n",
    "x_test_feed = x_test[params]\n",
    "y_train_feed = list(y_train.returned)\n",
    "#y_test_feed = list(y_test)\n",
    "\n",
    "x1, x2, y1, y2 = train_test_split(x_train_feed, y_train_feed, test_size = 0.3)\n",
    "\n",
    "assert(set(list(x_train_feed.columns)) == set(list(x_test_feed.columns)))\n",
    "assert(set(list(x1.columns)) == set(list(x2.columns)))\n",
    "assert(type(y1) == list)\n",
    "assert(type(y2) == list)\n",
    "assert(len(y1) == len(x1.index))\n",
    "assert(len(y2) == len(x2.index))\n",
    "\n",
    "#list(x1.columns), list(x2.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.14828533533008867, 5.968204828671742) \n",
      " (4.2612866421946283, 5.9818595984148892) \n",
      " (5.7578166911735513, 5.6859504321656482)\n"
     ]
    }
   ],
   "source": [
    "RF = ML(x1, x2, y1, y2, ensemble.RandomForestClassifier(n_estimators=400, max_features=7))\n",
    "GB = ML(x1, x2, y1, y2, ensemble.GradientBoostingClassifier(n_estimators=600))\n",
    "XGB = ML(x1, x2, y1, y2, xgb.XGBClassifier())\n",
    "\n",
    "\n",
    "print(RF.log_loss, \"\\n\", GB.log_loss, \"\\n\", XGB.log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "out = GB.predict(x_test_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.ensemble.forest.RandomForestClassifier"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = ensemble.RandomForestClassifier(n_estimators=100, max_features=5)\n",
    "type(ensemble.RandomForestClassifier(n_estimators=100, max_features=5))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature importances:\")\n",
    "for f, idx in enumerate(indices):\n",
    "    print(\"{:2d}. feature '{:5s}' ({:.4f})\".format(f + 1, params[idx], importances[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
