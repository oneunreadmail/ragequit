{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import model_selection, ensemble\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xytte = pd.read_csv(\"data/xytte.csv\")\n",
    "\n",
    "####################### ВНИМАНИЕ #######################\n",
    "# здесь трейн это которые в задаче трейн, с ответами\n",
    "# тест без ответов\n",
    "# но дальше в алгоритмах трейн будет разбиваться на два подсета\n",
    "# которые будут у меня называться x1-y1 и x2-y2, а по смыслу как раз трейн и тест\n",
    "# надо не перепутать\n",
    "\n",
    "x_train = xytte[xytte.returned == xytte.returned].reset_index().drop(\"returned\", axis=1)\n",
    "y_train = xytte[xytte.returned == xytte.returned].reset_index()[[\"returned\"]]\n",
    "x_test  = xytte[xytte.returned != xytte.returned].reset_index().drop(\"returned\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# проверяем, что нигде не налажали\n",
    "# читаем из файла оригинальные данные\n",
    "\n",
    "x_test_ref = pd.read_csv(\"data/x_test.csv\", sep=\";\", dtype=np.float32)\n",
    "x_train_ref = pd.read_csv(\"data/x_train.csv\", sep=\";\", dtype=np.float32)\n",
    "y_train_ref = pd.read_csv(\"data/y_train.csv\", sep=\";\", dtype=np.float32, header=None, names=[\"returned\"])\n",
    "\n",
    "# заводим функцию на сравнение таблиц\n",
    "\n",
    "def df_equal(df1, df2):\n",
    "    if list(df1.columns) != list(df2.columns): \n",
    "        print(\"Mismatch columns:\")\n",
    "        print(\"df1: \", list(df1.columns))\n",
    "        print(\"df2: \", list(df2.columns))\n",
    "        return False\n",
    "    if list(df1.index) != list(df2.index): \n",
    "        print(\"Mismatch index:\")\n",
    "        print(\"df1: \", list(df1.columns)[0], \":\", list(df1.columns)[-1])\n",
    "        print(\"df2: \", list(df2.columns)[0], \":\", list(df2.columns)[-1])\n",
    "        return False\n",
    "    \n",
    "    ne_stacked = (df1 != df2).stack()\n",
    "    changed = ne_stacked[ne_stacked]\n",
    "    changed.index.names = ['id', 'col']\n",
    "\n",
    "    difference_locations = np.where(df1 != df2)\n",
    "    changed_from = df1.values[difference_locations]\n",
    "    changed_to = df2.values[difference_locations]\n",
    "    diff = pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)\n",
    "    return not diff[diff[\"from\"] - diff[\"to\"] > 0.000001].count()[0]\n",
    "\n",
    "# ассертим наши таблицы\n",
    "\n",
    "assert(df_equal(x_test[x_test_ref.columns], x_test_ref))\n",
    "assert(df_equal(x_train[x_train_ref.columns], x_train_ref))\n",
    "assert(df_equal(y_train, y_train_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params_orig = [\n",
    "     'maxPlayerLevel',\n",
    "     'numberOfAttemptedLevels',\n",
    "     'attemptsOnTheHighestLevel',\n",
    "     'totalNumOfAttempts',\n",
    "     'averageNumOfTurnsPerCompletedLevel',\n",
    "     'numberOfBoostersUsed',\n",
    "     'fractionOfUsefullBoosters',\n",
    "     'totalScore',\n",
    "     'totalBonusScore',\n",
    "     'totalStarsCount',\n",
    "     'numberOfDaysActuallyPlayed',\n",
    "]\n",
    "     \n",
    "params_mod = [\n",
    "     #'attemptsOnTheHighestLevel_dvd',\n",
    "     'attemptsOnTheHighestLevel_ln',\n",
    "     'attemptsPerDay',\n",
    "     'averageNumOfTurnsPerCompletedLevel_dvd',\n",
    "     'maxPlayerLevel_ln',\n",
    "     'numberOfAttemptedLevels_dvd',\n",
    "     #'numberOfBoostersUsed_dvd',\n",
    "     'numberOfBoostersUsed_dvd_ln',\n",
    "     #'numberOfBoostersUsed_ln',\n",
    "     'numberOfDaysActuallyPlayed_ln',\n",
    "     'totalBonusScore_dvd',\n",
    "     'totalNumOfAttempts_ln',\n",
    "     #'totalScore_ln',\n",
    "     'totalScore_ln_dvd',\n",
    "     #'totalStarsCount_dvd',\n",
    "     'totalStarsCount_dvd_ln',\n",
    "     #'totalStarsCount_ln',\n",
    "]\n",
    "\n",
    "params_bool = [\n",
    "     'allAttemptsOnTheHighestLevel',\n",
    "     'attLevelsMoreThanMaxLevel',\n",
    "     'doReturnOnLowerLevels',\n",
    "     'zeroTotalScore',\n",
    "     'zeroTurnsPerCompletedLevel'\n",
    "]\n",
    "\n",
    "params_mod_norm = [\n",
    "     'attemptsOnTheHighestLevel_dvd_norm',\n",
    "     #'attemptsOnTheHighestLevel_ln_norm',\n",
    "     #'attemptsOnTheHighestLevel_norm',\n",
    "     'attemptsPerDay_norm',\n",
    "     'averageNumOfTurnsPerCompletedLevel_dvd_norm',\n",
    "     #'averageNumOfTurnsPerCompletedLevel_norm',\n",
    "     'fractionOfUsefullBoosters_norm',\n",
    "     'maxPlayerLevel_ln_norm',\n",
    "     #'maxPlayerLevel_norm',\n",
    "     'numberOfAttemptedLevels_dvd_norm',\n",
    "     #'numberOfAttemptedLevels_norm',\n",
    "     'numberOfBoostersUsed_dvd_ln_norm',\n",
    "     #'numberOfBoostersUsed_dvd_norm',\n",
    "     #'numberOfBoostersUsed_ln_norm',\n",
    "     #'numberOfBoostersUsed_norm',\n",
    "     'numberOfDaysActuallyPlayed_ln_norm',\n",
    "     #'numberOfDaysActuallyPlayed_norm',\n",
    "     'totalBonusScore_dvd_norm',\n",
    "     #'totalBonusScore_norm',\n",
    "     'totalNumOfAttempts_ln_norm',\n",
    "     #'totalNumOfAttempts_norm',\n",
    "     'totalScore_ln_dvd_norm',\n",
    "     #'totalScore_ln_norm',\n",
    "     #'totalScore_norm',\n",
    "     'totalStarsCount_dvd_ln_norm',\n",
    "     #'totalStarsCount_dvd_norm',\n",
    "     'totalStarsCount_ln_norm',\n",
    "     #'totalStarsCount_norm',\n",
    "]\n",
    "\n",
    "params = params_orig + [\"doReturnOnLowerLevels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17702 7587\n"
     ]
    }
   ],
   "source": [
    "x_train_feed = x_train[params]\n",
    "x_test_feed = x_test[params]\n",
    "y_train_feed = list(y_train.returned)\n",
    "#y_test_feed = list(y_test)\n",
    "\n",
    "x1, x2, y1, y2 = train_test_split(x_train_feed, y_train_feed, test_size = 0.3)\n",
    "\n",
    "N_train, _ = x1.shape \n",
    "N_test,  _ = x2.shape \n",
    "print(N_train, N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00982939780816 0.183998945565\n",
      "0.00982939780816 0.186898642415\n",
      "0.00960343464015 0.186371424806\n",
      "0.00519715286408 0.180967444313\n",
      "0.00525364365608 0.180176617899\n",
      "0.00519715286408 0.186239620403\n",
      "0.00485820811208 0.17846316067\n",
      "0.00474522652808 0.179122182681\n",
      "0.00480171732008 0.182153683933\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "estimators_list = [20, 50, 100]\n",
    "features_list = [3, 5, 7]\n",
    "for estimators in estimators_list:\n",
    "    for features in features_list:\n",
    "        rf = ensemble.RandomForestClassifier(n_estimators=estimators, random_state=35, max_features=features)\n",
    "        rf.fit(x1, y1)\n",
    "        err_train = np.mean(y1 != rf.predict(x1))\n",
    "        err_test  = np.mean(y2  != rf.predict(x2))\n",
    "        print(err_train, err_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances:\n",
      " 1. feature 'numberOfDaysActuallyPlayed' (0.2684)\n",
      " 2. feature 'totalNumOfAttempts' (0.1543)\n",
      " 3. feature 'averageNumOfTurnsPerCompletedLevel' (0.1088)\n",
      " 4. feature 'totalScore' (0.1074)\n",
      " 5. feature 'maxPlayerLevel' (0.0861)\n",
      " 6. feature 'totalBonusScore' (0.0633)\n",
      " 7. feature 'numberOfBoostersUsed' (0.0480)\n",
      " 8. feature 'totalStarsCount' (0.0465)\n",
      " 9. feature 'fractionOfUsefullBoosters' (0.0434)\n",
      "10. feature 'attemptsOnTheHighestLevel' (0.0418)\n",
      "11. feature 'numberOfAttemptedLevels' (0.0245)\n",
      "12. feature 'doReturnOnLowerLevels' (0.0075)\n"
     ]
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature importances:\")\n",
    "for f, idx in enumerate(indices):\n",
    "    print(\"{:2d}. feature '{:5s}' ({:.4f})\".format(f + 1, params[idx], importances[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.162354536211 0.174245419797\n"
     ]
    }
   ],
   "source": [
    "# GRAD BOOST\n",
    "\n",
    "gbt = ensemble.GradientBoostingClassifier(n_estimators=100, random_state=11)\n",
    "gbt.fit(x1, y1)\n",
    "\n",
    "err_train = np.mean(y1 != gbt.predict(x1))\n",
    "err_test = np.mean(y2 != gbt.predict(x2))\n",
    "print(err_train, err_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.77%\n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(x1, y1)\n",
    "\n",
    "y_pred = model.predict(x2)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y2, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "#log_loss(y_test_1D, predictions, eps=1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
